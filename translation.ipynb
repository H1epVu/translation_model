{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz\n",
      "  Using cached https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz (233.3 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from vi_core_news_lg==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->vi_core_news_lg==3.6.0) (2.1.3)\n",
      "⚠ As of spaCy v3.0, model symlinks are not supported anymore. You can load\n",
      "trained pipeline packages using their full names or from a directory path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The command 'link' is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 435.7 kB/s eta 0:00:30\n",
      "     --------------------------------------- 0.1/12.8 MB 469.7 kB/s eta 0:00:28\n",
      "      --------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:13\n",
      "      --------------------------------------- 0.3/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.6/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.0/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.2/12.8 MB 19.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.5/12.8 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 31.2 MB/s eta 0:00:00\n",
      "Collecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==3.7.1)\n",
      "  Downloading spacy-3.7.5-cp38-cp38-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading thinc-8.2.4-cp38-cp38-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yoshi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Downloading spacy-3.7.5-cp38-cp38-win_amd64.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.5 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.2/12.5 MB 2.3 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.3/12.5 MB 2.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.5/12.5 MB 3.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.4/12.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.3/12.5 MB 19.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.5 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.5 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.5/12.5 MB 19.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.4-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 32.6 MB/s eta 0:00:00\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.12\n",
      "    Uninstalling thinc-8.1.12:\n",
      "      Successfully uninstalled thinc-8.1.12\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.6.1\n",
      "    Uninstalling spacy-3.6.1:\n",
      "      Successfully uninstalled spacy-3.6.1\n",
      "Successfully installed spacy-3.7.4 thinc-8.2.3\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\AppData\\Local\\Temp\\pip-uninstall-e4re6rza'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\thinc\\~ackends'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\thinc\\~xtra'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\AppData\\Local\\Temp\\pip-uninstall-0ilkbxhd'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\spacy\\~b'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\spacy\\~atcher'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\spacy\\~l'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\spacy\\~ipeline'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yoshi\\anaconda3\\Lib\\site-packages\\spacy\\~okens'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vi-core-news-lg 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip -q install pyvi\n",
    "! pip install https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz\n",
    "! python -m spacy link vi_spacy_model vi_spacy_model\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=200, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x*math.sqrt(self.d_model)\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
    "\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "            \n",
    "        # cộng embedding vector với pe\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "\n",
    "    # attention score được tính bằng cách nhân q với k\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "    # xong rồi thì chuẩn hóa bằng softmax\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//heads\n",
    "        self.h = heads\n",
    "        self.attn = None\n",
    "\n",
    "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v\n",
    "\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # tính attention score\n",
    "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
    "\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = d_model\n",
    "\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Trong kiến trúc của chúng ta có tầng linear\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        # tính attention value, các bạn để ý q, k, v là giống nhau\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        \n",
    "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        trg: batch_size x seq_length\n",
    "        e_outputs: batch_size x seq_length x d_model\n",
    "        src_mask: batch_size x 1 x seq_length\n",
    "        trg_mask: batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x d_model\n",
    "        \"\"\"\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        src: batch_size x seq_length\n",
    "        trg: batch_size x seq_length\n",
    "        src_mask: batch_size x 1 x seq_length\n",
    "        trg_mask batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x vocab_size\n",
    "        \"\"\"\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, device):\n",
    "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
    "     mô hình không nhìn thấy được các từ ở tương lai\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    np_mask = np_mask.to(device)\n",
    "\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, src_pad, trg_pad, device):\n",
    "    \"\"\" Tạo mask cho encoder,\n",
    "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
    "    \"\"\"\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, device)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
    "    \"\"\"\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "\n",
    "    # tính sẵn output của encoder\n",
    "    e_output = model.encoder(src, src_mask)\n",
    "\n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "\n",
    "    outputs = outputs.to(device)\n",
    "\n",
    "    trg_mask = nopeak_mask(1, device)\n",
    "    # dự đoán kí tự đầu tiên\n",
    "    out = model.out(model.decoder(outputs,\n",
    "    e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(k, max_len).long()\n",
    "    outputs = outputs.to(device)\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
    "\n",
    "    e_outputs = e_outputs.to(device)\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, device, k, max_len):\n",
    "\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, device)\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:,:i],\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
    "\n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "\n",
    "        if num_finished_sentences == k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "\n",
    "    if ind is None:\n",
    "\n",
    "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "\n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\"Dịch một câu sử dụng beamsearch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "    sentence = sentence.to(device)\n",
    "\n",
    "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
    "\n",
    "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "class tokenize(object):\n",
    "\n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "\n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(src_file, trg_file):\n",
    "    src_data = open(src_file).read().strip().split('\\n')\n",
    "\n",
    "    trg_data = open(trg_file).read().strip().split('\\n')\n",
    "\n",
    "    return src_data, trg_data\n",
    "\n",
    "def create_fields(src_lang, trg_lang):\n",
    "\n",
    "    print(\"loading spacy tokenizers...\")\n",
    "\n",
    "    t_src = tokenize(src_lang)\n",
    "    t_trg = tokenize(trg_lang)\n",
    "\n",
    "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
    "\n",
    "    return SRC, TRG\n",
    "\n",
    "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    if istrain:\n",
    "        SRC.build_vocab(train)\n",
    "        TRG.build_vocab(train)\n",
    "\n",
    "    return train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(model, optimizer,batch, criterion):\n",
    "    \"\"\"\n",
    "    Một lần cập nhật mô hình\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    src = batch.src.transpose(0,1).cuda()\n",
    "    trg = batch.trg.transpose(0,1).cuda()\n",
    "    trg_input = trg[:, :-1]\n",
    "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "    preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "    ys = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "    loss.backward()\n",
    "    optimizer.step_and_update_lr()\n",
    "\n",
    "    loss = loss.item()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validiate(model, valid_iter, criterion):\n",
    "    \"\"\" Tính loss trên tập validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        for batch in valid_iter:\n",
    "            src = batch.src.transpose(0,1).cuda()\n",
    "            trg = batch.trg.transpose(0,1).cuda()\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "\n",
    "            loss = loss.item()\n",
    "\n",
    "            total_loss.append(loss)\n",
    "\n",
    "    avg_loss = np.mean(total_loss)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "    def state_dict(self):\n",
    "        optimizer_state_dict = {\n",
    "            'init_lr':self.init_lr,\n",
    "            'd_model':self.d_model,\n",
    "            'n_warmup_steps':self.n_warmup_steps,\n",
    "            'n_steps':self.n_steps,\n",
    "            '_optimizer':self._optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        return optimizer_state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.init_lr = state_dict['init_lr']\n",
    "        self.d_model = state_dict['d_model']\n",
    "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
    "        self.n_steps = state_dict['n_steps']\n",
    "\n",
    "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen):\n",
    "    pred_sents = []\n",
    "    for sentence in valid_src_data:\n",
    "        pred_trg = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen)\n",
    "        pred_sents.append(pred_trg)\n",
    "\n",
    "    pred_sents = [TRG.preprocess(sent) for sent in pred_sents]\n",
    "    trg_sents = [[sent.split()] for sent in valid_trg_data]\n",
    "\n",
    "    return bleu_score(pred_sents, trg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'train_src_data':'./data/train.en',\n",
    "    'train_trg_data':'./data/train.vi',\n",
    "    'valid_src_data':'./data/tst2013.en',\n",
    "    'valid_trg_data':'./data/tst2013.vi',\n",
    "    'src_lang':'en_core_web_sm',\n",
    "    'trg_lang':'en_core_web_sm',#'vi_spacy_model',\n",
    "    'max_strlen':160,\n",
    "    'batchsize':1500,\n",
    "    'device':'cuda',\n",
    "    'd_model': 512,\n",
    "    'n_layers': 6,\n",
    "    'heads': 8,\n",
    "    'dropout': 0.1,\n",
    "    'lr':0.0001,\n",
    "    'epochs':30,\n",
    "    'printevery': 200,\n",
    "    'k':5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy tokenizers...\n",
      "creating dataset and iterator... \n",
      "creating dataset and iterator... \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
    "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
    "\n",
    "SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n",
    "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
    "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad = SRC.vocab.stoi['<pad>']\n",
    "trg_pad = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(opt['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = ScheduledOptim(\n",
    "#         torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "#         0.2, opt['d_model'], 4000)\n",
    "\n",
    "# criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for epoch in range(opt['epochs']):\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for i, batch in enumerate(train_iter): \n",
    "#         s = time.time()\n",
    "#         loss = step(model, optimizer, batch, criterion)\n",
    "        \n",
    "#         total_loss += loss\n",
    "        \n",
    "#         if (i + 1) % opt['printevery'] == 0:\n",
    "#             avg_loss = total_loss/opt['printevery']\n",
    "#             print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(epoch, i, avg_loss, time.time()- s))\n",
    "#             total_loss = 0\n",
    "\n",
    "#     s = time.time()\n",
    "#     valid_loss = validiate(model, valid_iter, criterion)\n",
    "#     bleuscore = bleu(valid_src_data[:500], valid_trg_data[:500], model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
    "#     print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - time: {:.4f}'.format(epoch, i, valid_loss, bleuscore, time.time() - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('D:/.study/KHMT/translation_model/savedModel/transformer.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tôi là một người cảnh sát'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='i am a police'\n",
    "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
    "trans_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def on_translate():\n",
    "    sentence = text.get(\"1.0\", tk.END).strip()\n",
    "    trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
    "    output_text.config(state=tk.NORMAL)\n",
    "    output_text.delete(\"1.0\", tk.END)\n",
    "    output_text.insert(tk.END, trans_sent)\n",
    "    output_text.config(state=tk.DISABLED)\n",
    "root = tk.Tk()\n",
    "root.title(\"Translator\")\n",
    "\n",
    "root.geometry(\"400x400\")\n",
    "\n",
    "input_frame = tk.Frame(root)\n",
    "input_frame.pack(expand=True, pady=10, fill='both')\n",
    "\n",
    "label = tk.Label(input_frame, text=\"Input sentence:\")\n",
    "label.pack(side=\"top\", padx=(10, 5), pady=5)\n",
    "\n",
    "text = tk.Text(input_frame, height=5, width=50)\n",
    "text.pack(side=\"top\", padx=(10, 10), pady=5, fill=\"both\", expand=True)\n",
    "\n",
    "translate_button = tk.Button(root, text=\"Translate\", command=on_translate)\n",
    "translate_button.pack(pady=5)\n",
    "\n",
    "output_frame = tk.Frame(root)\n",
    "output_frame.pack(expand=True, pady=10, fill='both')\n",
    "\n",
    "output_label = tk.Label(output_frame, text=\"Translated sentence:\")\n",
    "output_label.pack(side=\"top\", padx=(10, 5), pady=5)\n",
    "\n",
    "output_text = tk.Text(output_frame, height=5, width=50, state=tk.DISABLED)\n",
    "output_text.pack(side=\"top\", padx=(10, 10), pady=5, fill=\"both\", expand=True)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
